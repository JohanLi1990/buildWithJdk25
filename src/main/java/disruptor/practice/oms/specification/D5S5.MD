Cool. Your `TaskObject` refactor is in the right direction (immutable + carries `taskType` + `rejReason`). Two tiny tweaks I’d do later (not blocking Session 5): make `rejReason` `null` instead of `""` to avoid allocating empty strings, and consider a dedicated “invalid” constructor to avoid carrying payload for invalid frames. But you can keep it as-is for now.

Below is a **worker-side pseudocode** walk-through for **Stage1 → Stage4**, including **latency stamps** and the **single-response rule**.

---

# Worker pipeline pseudocode (Stage1–Stage4 + latency)

### Data you already have

* `TaskObject t` (immutable, contains `t0` in `TaskEvent` today — you may want to carry `t0` into `TaskObject` too)
* `Channel ch = t.channel`
* `now = System.nanoTime()`

> **Recommendation:** carry `t0` into `TaskObject`. Otherwise you can still read it from the original `TaskEvent`, but then `TaskObject` isn’t self-contained for latency.

---

## Common structs (conceptual)

```
StageCtx:
  t0  // receive time (netty)
  t1  // after stage1
  t2  // after stage2
  t3  // after stage3
  t4  // after stage4 (response built, before enqueue write)
  decision: ACCEPT | REJECT
  rejCode: string  (e.g. BAD_MSG, RISK_LIMIT)
  parsed: ParsedOrder?   // output of stage1 business parse
  execResult: ExecSim?   // output of stage3
  response: TaskResponse // final output of stage4
```

---

# Stage 1 — “semantic validation + business parse”

**Goal:** turn payload into structured command OR reject early.

```
stage1(t, ctx):
  // 0) decoder-classified invalid message shortcut
  if t.taskType == INVALID_MSG:
      ctx.decision = REJECT
      ctx.rejCode = t.rejReason (or "BAD_MSG")
      ctx.t1 = now()
      return ctx

  // 1) basic semantic checks
  if t.payload == null or t.payload is empty:
      ctx.decision = REJECT
      ctx.rejCode = "BAD_MSG:EMPTY_PAYLOAD"
      ctx.t1 = now()
      return ctx

  // 2) business parse payload (example "BUY 100 TSLA")
  tokens = splitBySpace(t.payload)           // keep simple for now
  if tokens.size != 3:
      ctx.decision = REJECT
      ctx.rejCode = "BAD_MSG:BAD_PAYLOAD_FMT"
      ctx.t1 = now()
      return ctx

  side = tokens[0]; qty = tokens[1]; sym = tokens[2]
  if side not in {BUY, SELL}:
      reject BAD_MSG:SIDE
  if qty not numeric or qty <= 0:
      reject BAD_MSG:QTY

  ctx.parsed = ParsedOrder(side, qty, sym)
  ctx.decision = ACCEPT
  ctx.t1 = now()
  return ctx
```

Latency captured:

* `t1 - t0` = parse/validate time (plus earlier steps)

---

# Stage 2 — “risk checks”

**Goal:** reject with `RISK_*` codes.

```
stage2(t, ctx):
  if ctx.decision == REJECT:
      ctx.t2 = now()
      return ctx

  // mock risk rules
  if ctx.parsed.qty > 10_000:
      ctx.decision = REJECT
      ctx.rejCode = "RISK_MAX_QTY"
      ctx.t2 = now()
      return ctx

  if ctx.parsed.symbol in { "SCAM", "XYZ0" }:
      ctx.decision = REJECT
      ctx.rejCode = "RISK_BANNED_SYMBOL"
      ctx.t2 = now()
      return ctx

  // otherwise OK
  ctx.t2 = now()
  return ctx
```

Latency captured:

* `t2 - t1` = risk stage time

---

# Stage 3 — “execution simulation”

**Goal:** only run if ACCEPT; emulate a downstream call.

```
stage3(t, ctx):
  if ctx.decision == REJECT:
      ctx.t3 = now()
      return ctx

  // mock downstream cost (10–20ms)
  // (later: don't sleep in real p99 benchmarking, but OK for mock)
  execStart = now()
  simulateExecDelay(10..20 ms)
  ctx.execResult = ExecSim(fillPrice=..., status="ACK")
  ctx.t3 = now()
  return ctx
```

Latency captured:

* `t3 - t2` = exec stage time (dominant)

---

# Stage 4 — “build exactly one response”

**Goal:** ALWAYS build one `TaskResponse` no matter what happened.

```
stage4(t, ctx):
  // build response exactly once
  if ctx.decision == REJECT:
      ctx.response = TaskResponse.rej(
          orderId=t.orderId,
          correlationId=t.correlationId,
          seqInFamily=t.seqInFamily,
          reason=ctx.rejCode,
          t0=ctx.t0,
          t4=now()
      )
  else:
      ctx.response = TaskResponse.ack(
          orderId=t.orderId,
          correlationId=t.correlationId,
          seqInFamily=t.seqInFamily,
          payload="ACK:" + ctx.execResult.fillPrice,
          t0=ctx.t0,
          t4=now()
      )

  ctx.t4 = now()
  return ctx
```

Latency captured:

* `t4 - t3` = response build time

---

# Outbound write-back (single safe path)

**Rule:** worker thread never calls `writeAndFlush` directly.

```
writeBack(t, ctx):
  ch = t.channel

  // marshal to event loop thread
  ch.eventLoop().execute(() -> {
      ch.writeAndFlush(ctx.response)
  })

  // important: DO NOT wait for flush future
```

---

# End-to-end latency metrics you can log

At the end of worker runnable:

```
e2e = ctx.t4 - ctx.t0     // until response built (business latency)
s1  = ctx.t1 - ctx.t0
s2  = ctx.t2 - ctx.t1
s3  = ctx.t3 - ctx.t2
s4  = ctx.t4 - ctx.t3

logDebugOrMetrics(
  orderId, correlationId, seq,
  decision, rejCode,
  e2e, s1, s2, s3, s4
)
```

**Note:** If you also want “network flush latency”, you can timestamp inside the event-loop runnable, but keep it *separate* from the business pipeline.

---

# Two practical integration notes (so it doesn’t break scheduler)

1. **Release family after Stage4 is done and write is enqueued**

* Build response → enqueue write → release family → drain next
* Don’t tie release to flush completion

2. **Carry `t0` into `TaskObject`**
   Right now `TaskObject` doesn’t have `t0`, but your latency measurement needs it. I’d add:

```java
private final long t0;
```

and set it in constructors.

---

If you paste your current `dispatchAndDrain(...)` worker runnable body (just that method), I’ll help you wire this pseudocode into your exact structure without changing the scheduler invariants you already tested.
